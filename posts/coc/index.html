<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Consistent Ring Within a Consistent Ring | Open Notes</title><meta name=description content="Minimal notes"><link rel=canonical href=https://nagarajrpoojari.github.io/opennotes/posts/coc/><meta property="og:title" content="Consistent Ring Within a Consistent Ring"><meta property="og:description" content="Minimal notes"><meta property="og:type" content="article"><meta property="og:url" content="https://nagarajrpoojari.github.io/opennotes/posts/coc/"><meta name=twitter:card content="summary"><meta name=twitter:title content="Consistent Ring Within a Consistent Ring"><meta name=twitter:description content="Minimal notes"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","headline":"Consistent Ring Within a Consistent Ring","url":"https:\/\/nagarajrpoojari.github.io\/opennotes\/posts\/coc\/","datePublished":"2025-08-16T11:11:09\u002b05:30","dateModified":"2025-08-16T11:11:09\u002b05:30","author":{"@type":"Person","name":"Nagaraj"},"description":"Minimal notes"}</script><link rel=apple-touch-icon sizes=57x57 href=/opennotes/favicon_io/apple-icon-57x57.png><link rel=apple-touch-icon sizes=60x60 href=/opennotes/favicon_io/apple-icon-60x60.png><link rel=apple-touch-icon sizes=72x72 href=/opennotes/favicon_io/apple-icon-72x72.png><link rel=apple-touch-icon sizes=76x76 href=/opennotes/favicon_io/apple-icon-76x76.png><link rel=apple-touch-icon sizes=114x114 href=/opennotes/favicon_io/apple-icon-114x114.png><link rel=apple-touch-icon sizes=120x120 href=/opennotes/favicon_io/apple-icon-120x120.png><link rel=apple-touch-icon sizes=144x144 href=/opennotes/favicon_io/apple-icon-144x144.png><link rel=apple-touch-icon sizes=152x152 href=/opennotes/favicon_io/apple-icon-152x152.png><link rel=apple-touch-icon sizes=180x180 href=/opennotes/favicon_io/apple-icon-180x180.png><link rel=icon type=image/png sizes=192x192 href=/opennotes/favicon_io/android-icon-192x192.png><link rel=icon type=image/png sizes=32x32 href=/opennotes/favicon_io/favicon-32x32.png><link rel=icon type=image/png sizes=96x96 href=/opennotes/favicon_io/favicon-96x96.png><link rel=icon type=image/png sizes=16x16 href=/opennotes/favicon_io/favicon-16x16.png><link rel=manifest href=/opennotes/favicon_io/manifest.json><meta name=msapplication-TileColor content="#ffffff"><meta name=msapplication-TileImage content="/opennotes/favicon_io/ms-icon-144x144.png"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=/opennotes/css/main.min.383a7d1382f8498b5f1577cdce60b99b9a4f5c7b7aa954cbab4d0a7f85f73262.css integrity="sha256-ODp9E4L4SYtfFXfNzmC5m5pPXHt6qVTLq00Kf4X3MmI=" crossorigin=anonymous><link rel=stylesheet href=/opennotes/css/custom.min.da68636af3246ceae03c7f037fc6a3d62af522a10e883cac3ec8536110e0a023.css integrity="sha256-2mhjavMkbOrgPH8Df8aj1ir1IqEOiDysPshTYRDgoCM=" crossorigin=anonymous></head><body><a href=#main-content class=skip-link>Skip to content</a><header><nav class=path-nav><ol><li>/
<a href=/opennotes/>Open Notes</a>
/</li><li><a href=/opennotes/posts/>Posts</a>
/</li><li class=current><a href=/opennotes/posts/coc/>Consistent Ring Within a Consistent Ring</a></li></ol></nav></header><main id=main-content><h1>Consistent Ring Within a Consistent Ring</h1><div class=terms-list><ul><li><a href=/opennotes/tags/lsm-tree/>#LSM-Tree</a></li><li><a href=/opennotes/tags/consistent-hashing/>#Consistent-Hashing</a></li><li><a href=/opennotes/tags/replication/>#Replication</a></li><li><a href=/opennotes/tags/sharding/>#Sharding</a></li></ul></div><nav class=toc><strong>Table of contents</strong><div class=toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#revisiting-existing-designs>Revisiting Existing Designs</a></li><li><a href=#enter-orangedb>Enter OrangeDB</a></li><li><a href=#write-path>Write Path</a></li><li><a href=#read-path>Read Path</a></li><li><a href=#read-consistency-levels>Read Consistency Levels</a></li><li><a href=#what-orangedb-tries-to-solve>What OrangeDB Tries to Solve</a></li><li><a href=#final-thoughts>Final Thoughts</a></li></ul></li></ul></nav></div></nav><p>Over the past few months, I&rsquo;ve been diving deep into distributed storage internals — and somewhere along the way, <strong>OrangeDB</strong> was born. If you haven&rsquo;t heard of it yet, check <a href>orange</a>. This journey led me to explore several lesser-known databases that quietly power critical infrastructure in production environments.</p><p>Some highlights:</p><ul><li><strong>Cassandra</strong> — a fully leaderless, highly available system with tunable consistency.</li><li><strong>MongoDB</strong> — a semi-consistent document store with sharding and replica sets.</li><li><strong>LevelDB / RocksDB</strong> — high-performance embedded key-value stores.</li><li><strong>Voldemort / Riak</strong> — early pioneers in distributed key-value stores.</li></ul><h3 id=revisiting-existing-designs>Revisiting Existing Designs</h3><h4 id=cassandra><strong>Cassandra</strong></h4><p>Cassandra is completely <strong>leaderless</strong> — any node can handle reads or writes. Data is eventually replicated using <strong>gossip</strong>. To ensure consistency, Cassandra uses <strong>tunable quorum reads and writes</strong>:</p><ul><li>For a cluster with <code>n</code> replicas:<ul><li>A <strong>write</strong> must be acknowledged by at least <code>w</code> nodes.</li><li>A <strong>read</strong> must be satisfied by <code>r</code> nodes.</li><li>The constraint <code>w + r > n</code> ensures <strong>strong consistency</strong>.</li></ul></li></ul><p>Features like <strong>sloppy quorum</strong>, <strong>hinted handoff</strong>, and <strong>read repair</strong> further enhance availability. However, this design may result in <strong>higher read and write latencies</strong>, especially under quorum settings.</p><h4 id=mongodb-sharded-deployment><strong>MongoDB Sharded Deployment</strong></h4><p>In a MongoDB sharded cluster:</p><ul><li>The dataset is partitioned across <strong>shards</strong>.</li><li>Each shard is a <strong>replica set</strong> with a single <strong>primary</strong> and multiple <strong>secondaries</strong>.</li><li>All writes go to the <strong>primary</strong> of the relevant shard.</li><li>Reads can be routed to the <strong>primary</strong> or to secondaries depending on the configured read preference.</li></ul><p>This design offers <strong>strong consistency by default</strong> (via the primary) and <strong>lower read latency</strong> in relaxed consistency modes. However, it relies heavily on <strong>leader election (via Raft)</strong>, which introduces complexity (e.g., split-brain handling, election delays, leadership failover).</p><hr><h3 id=enter-orangedb>Enter OrangeDB</h3><p><figure><img src=coc-2.png alt=Screenshot loading=lazy><figcaption>Screenshot</figcaption></figure></p><p>OrangeDB attempts to combine the <strong>best of both worlds</strong>:</p><ul><li>The <strong>scalability and partitioning</strong> of MongoDB’s sharded setup.</li><li>The <strong>leaderless replication</strong> and tunable consistency of Cassandra.</li></ul><p>It introduces:</p><blockquote><h4 id=consistent-ring-within-a-consistent-ring>“Consistent Ring Within a Consistent Ring”</h4></blockquote><ul><li>An <strong>outer ring</strong> of nodes forms the basis for <strong>shard selection</strong> via consistent hashing.</li><li>Each shard contains an <strong>inner ring</strong> of <strong>replica nodes</strong>, also organized using consistent hashing.</li></ul><p>This two-layer ring structure enables <strong>fine-grained control</strong> over placement, replication, and load distribution.</p><hr><h3 id=write-path>Write Path</h3><ol><li>For a key like <code>k=120</code>, OrangeDB hashes the key to determine the target <strong>shard</strong> via the <strong>outer ring</strong>.</li><li>Within that shard, the key is again hashed to select a specific <strong>replica</strong> (let’s say <code>replica-1</code>) in the <strong>inner ring</strong>.</li><li>All writes for that key go to this designated <strong>primary replica</strong> — without any leader election.</li><li>That replica then <strong>asynchronously replicates</strong> the write to its sibling replicas within the shard using <strong>gossip-style replication</strong> (similar to Cassandra).</li></ol><p>This design achieves:</p><ul><li><strong>Write locality</strong> per key.</li><li><strong>Leaderless writes</strong> without the overhead of leader elections.</li><li><strong>Horizontal scalability</strong> by sharding at both levels.</li></ul><hr><h3 id=read-path>Read Path</h3><p>Reads can be performed at <strong>multiple consistency levels</strong> depending on the use case:</p><ul><li>A <strong>read</strong> first determines the target shard and replica (as with writes).</li><li>Based on the configured consistency level, the system will:<ul><li>Query <strong>only the primary replica</strong>, or</li><li>Query <strong>a quorum of replicas</strong>, or</li><li>Query <strong>all replicas</strong> and enforce unanimous agreement.</li></ul></li></ul><hr><h3 id=read-consistency-levels>Read Consistency Levels</h3><p>OrangeDB supports three read modes:</p><h5 id=all><em><code>all</code></em></h5><ul><li>Queries <strong>all replicas</strong> of a shard.</li><li>Requires <strong>unanimous agreement</strong> on the value.</li><li><strong>Strongest consistency</strong>, but higher latency.</li><li>Fails if any replica is out of sync.</li></ul><h5 id=quorum><em><code>quorum</code></em></h5><ul><li>Queries all replicas, but only a <strong>majority</strong> need to agree.</li><li>Balances <strong>availability and consistency</strong>.</li><li>Can optionally trigger <strong>read repair</strong> for divergent replicas.</li></ul><h5 id=single><em><code>single</code></em></h5><ul><li>Reads from the <strong>designated primary replica</strong> only.</li><li>Fastest option, but assumes the primary is <strong>available and up-to-date</strong>.</li><li>Risks <strong>stale reads</strong> or <strong>data loss</strong> if the replica is unavailable.</li></ul><hr><h3 id=what-orangedb-tries-to-solve>What OrangeDB Tries to Solve</h3><h6 id=orangedb-vs-cassandra-simpler-lower-latency-writes>OrangeDB vs Cassandra: Simpler, Lower-Latency Writes</h6><blockquote><p>OrangeDB avoids quorum-based writes by routing each key to a fixed replica, requiring no coordination during writes.</p></blockquote><p><strong>Why it matters:</strong></p><ul><li>Cassandra needs coordination between <code>w</code> replicas (e.g., 2 of 3) for every write.</li><li>OrangeDB writes to a single replica and replicates asynchronously — lower latency, less overhead.</li></ul><p><strong>Trade-off:</strong> Potential for data loss if the primary fails before replication. No immediate durability like Cassandra’s quorum model.</p><hr><h6 id=orangedb-vs-mongodb-no-leader-election-always-writable>OrangeDB vs MongoDB: No Leader Election, Always Writable</h6><blockquote><p>OrangeDB removes the concept of a primary per shard, avoiding Raft-based elections and their downtime.</p></blockquote><p><strong>Why it matters:</strong></p><ul><li>MongoDB pauses writes during primary failover and risks split-brain under partitions.</li><li>OrangeDB uses deterministic key-to-replica mapping — no election, no downtime.</li></ul><p><strong>Trade-off:</strong> Requires smarter client or routing logic to handle failed replicas gracefully.</p><hr><hr><h3 id=final-thoughts>Final Thoughts</h3><p>OrangeDB started as a personal exploration—an educational project driven by curiosity about how distributed storage systems work under the hood. It’s a playground to experiment with ideas inspired by Cassandra, MongoDB, and others, but rethinking some of their core trade-offs.</p><p>This project is far from production-ready. There are still many unanswered questions around durability, failure handling, and consistency guarantees. But that’s part of the fun—learning by building and seeing what challenges arise.</p><p>If you’ve enjoyed this peek into the inner workings of distributed databases, I hope it sparks your own experiments and deep dives. At the end of the day, OrangeDB is just one step in an ongoing journey to understand and improve how I store and manage data at scale.</p><p>Thanks for reading and sharing the curiosity! ✌️</p><div class=time><time datetime=2025-08-16>2025-08-16&nbsp;</time></div><div class=comments></div><div class=terminal-nav><div class=back-nav><a href=/opennotes/posts/ class=back-link>../</a></div></div></main><footer><p>&copy; Copyright 2026 &#183;
<a href=https://github.com/ntk148v/shibui>shibui</a></p></footer></body></html>